{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chess tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, losses, callbacks\n",
    "\n",
    "# Define the evaluation function model\n",
    "def create_evaluation_model():\n",
    "    model = keras.Sequential([\n",
    "        #input layer\n",
    "        keras.layers.Dense(132, activation='relu', input_shape=[132], kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        #hidden layers\n",
    "        #keras.layers.Dense(4096, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        #keras.layers.Dense(2048, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "        #keras.layers.Dense(2048, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "\n",
    "        #output layer\n",
    "        keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    # Set the laerning rate\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Choose the optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    #optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    #optimizer = optimizers.SGD(learning_rate=learning_rate)\n",
    "    #optimizer = optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    \n",
    "    # Choose the loss function\n",
    "    #loss=losses.mean_absolute_error\n",
    "    loss=losses.mean_squared_error\n",
    "    #loss=losses.binary_crossentropy\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert FEN to a feature vector\n",
    "def fen_to_features(fen):\n",
    "    board = chess.Board(fen)\n",
    "    features = np.zeros(132)  # Increase the feature vector size to accommodate additional features\n",
    "    \n",
    "    for i in range(64):\n",
    "        piece = board.piece_at(i)\n",
    "        if piece is not None:\n",
    "            piece_type = piece.piece_type\n",
    "            color = piece.color\n",
    "            sign = 1 if color == chess.WHITE else -1\n",
    "            features[i] = sign * piece_type\n",
    "    \n",
    "    # Additional features\n",
    "    features[64] = int(board.turn)  # Current player's turn (0 for black, 1 for white)\n",
    "    \n",
    "   # Piece mobility\n",
    "    for i in range(64):\n",
    "        piece = board.piece_at(i)\n",
    "        if piece is not None:\n",
    "            piece_mobility = len(board.attacks(i))\n",
    "            features[i + 65] = piece_mobility\n",
    "    \n",
    "    # King safety\n",
    "    white_king_safety = calculate_king_safety(board, chess.WHITE)\n",
    "    black_king_safety = calculate_king_safety(board, chess.BLACK)\n",
    "    features[129] = white_king_safety\n",
    "    features[130] = black_king_safety\n",
    "\n",
    "    # Forced mate flag\n",
    "    if board.is_checkmate():\n",
    "        if board.turn == chess.WHITE and board.result() == \"1-0\":\n",
    "            features[131] = 1.0\n",
    "        elif board.turn == chess.BLACK and board.result() == \"0-1\":\n",
    "            features[131] = -1.0\n",
    "    \n",
    "    return features\n",
    "    \n",
    "def is_forced_mate(eval_score, color):\n",
    "    if color == chess.WHITE and eval_score >= 4000:\n",
    "        return True\n",
    "    elif color == chess.BLACK and eval_score <= -4000:\n",
    "        return True\n",
    "    return False    \n",
    "\n",
    "def calculate_king_safety(board, color):\n",
    "    king_square = board.king(color)\n",
    "    king_safety = 0\n",
    "    \n",
    "    # Example: Evaluate king safety by counting the number of squares attacked by opponent's pieces\n",
    "    opponent_color = not color\n",
    "    for i in range(64):\n",
    "        if board.is_attacked_by(opponent_color, i):\n",
    "            king_safety += 1\n",
    "    \n",
    "    return king_safety\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_position(model, position):\n",
    "    eval_score = model.predict(np.array([position]))[0][0]\n",
    "    return eval_score\n",
    "\n",
    "def is_forced_mate(eval_score, color):\n",
    "    if color == chess.WHITE and eval_score >= 4000:\n",
    "        return True\n",
    "    elif color == chess.BLACK and eval_score <= -4000:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Read training data from CSV\n",
    "training_data = pd.read_csv('/kaggle/input/chesstraindata/chessData-10L.csv')\n",
    "\n",
    "# Convert training data to feature vectors\n",
    "training_positions = []\n",
    "training_scores = []\n",
    "\n",
    "for i in range(len(training_data['FEN'])):\n",
    "    fen = training_data['FEN'][i]\n",
    "    position = fen_to_features(fen)\n",
    "    score = float(training_data['eval'][i])\n",
    "    \n",
    "    if not is_forced_mate(score, chess.WHITE) and not is_forced_mate(score, chess.BLACK):\n",
    "        training_positions.append(position)\n",
    "        training_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the evaluation function model\n",
    "model = create_evaluation_model()\n",
    "\n",
    "# Define the ProgbarLogger callback\n",
    "#progbar_logger = keras.callbacks.ProgbarLogger()\n",
    "\n",
    "# Fit the model with the ProgbarLogger callback\n",
    "#model.fit(np.array(training_positions), np.array(training_scores), epochs=500, batch_size=1048575, callbacks=[progbar_logger])\n",
    "model.fit(np.array(training_positions), np.array(training_scores), epochs=35, batch_size=20972)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load the model\n",
    "model.save('DeepCarlsen.h5')\n",
    "#model = keras.models.load_model('my_model.h5')\n",
    "model = keras.models.load_model('DeepCarlsen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "test_example = 'r2q1rk1/p1p1bppp/2p2n2/3p2B1/4P1b1/2NB1Q2/PPP2PPP/2KR3R w - - 8 11'\n",
    "# A position from a game played by me as black with eval = -1.17, although both side have equal values of pieces\n",
    "\n",
    "# Convert test example to feature vector\n",
    "test_position = fen_to_features(test_example)\n",
    "\n",
    "# Predict the evaluation score for the test example\n",
    "evaluation_score = evaluate_position(model, test_position)\n",
    "\n",
    "print('Test Example:', test_example)\n",
    "print('Evaluation Score:', evaluation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(position, depth, maximizing_player, evaluate_position):\n",
    "    if depth == 0 or position.is_game_over():\n",
    "        return evaluate_position(model, position)\n",
    "    \n",
    "    if maximizing_player:\n",
    "        max_eval = float('-inf')\n",
    "        for move in position.legal_moves:\n",
    "            position.push(move)\n",
    "            eval_score = minimax(position, depth - 1, False, evaluate_position)\n",
    "            position.pop()\n",
    "            max_eval = max(max_eval, eval_score)\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for move in position.legal_moves:\n",
    "            position.push(move)\n",
    "            eval_score = minimax(position, depth - 1, True, evaluate_position)\n",
    "            position.pop()\n",
    "            min_eval = min(min_eval, eval_score)\n",
    "        return min_eval\n",
    "\n",
    "\n",
    "def choose_best_move(position, depth):\n",
    "    best_move = None\n",
    "    best_eval = float('-inf')\n",
    "    for move in position.legal_moves:\n",
    "        position.push(move)\n",
    "        eval_score = minimax(position, depth - 1, False, evaluate_position)\n",
    "        position.pop()\n",
    "        if eval_score > best_eval:\n",
    "            best_eval = eval_score\n",
    "            best_move = move\n",
    "    return best_move\n",
    "\n",
    "# Example usage\n",
    "best_move = choose_best_move(test_position, depth=3)\n",
    "print(\"Best Move :\", best_move)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
